# news.py - Fixed version
import os
import time
import sys
from dotenv import load_dotenv
from crewai import Agent, Task, Crew, Process, LLM
from crewai_tools import SerperDevTool
import requests
from datetime import datetime
import warnings
import re
from bs4 import BeautifulSoup

# Suppress deprecation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Load environment variables
load_dotenv()

# Configure API keys
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")

# Suppress litellm verbose logging
os.environ["LITELLM_LOG"] = "ERROR"
import litellm
litellm.set_verbose = False

# Rate limiting configuration
class RateLimiter:
    def __init__(self, requests_per_minute=8):
        self.requests_per_minute = requests_per_minute
        self.min_interval = 60.0 / requests_per_minute
        self.last_request_time = 0
        
    def wait_if_needed(self):
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.min_interval:
            wait_time = self.min_interval - elapsed
            print(f"‚è≥ Rate limiting: Waiting {wait_time:.1f} seconds...")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()

# Global rate limiter - 8 requests per minute (safe for free tier)
rate_limiter = RateLimiter(requests_per_minute=8)

# Initialize the Gemini LLM
llm = LLM(
    model="gemini/gemini-1.5-flash",  # Using stable model
    api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.5,
    max_retries=3,
    timeout=60,
    max_rpm=8  # Rate limit per minute
)

# Add manager configuration for better coordination
manager_llm = LLM(
    model="gemini/gemini-2.0-flash-exp",
    api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.3,  # Lower temperature for more consistent management
    max_retries=3,
    timeout=60,
    max_rpm=8
)

# Initialize search tool
search_tool = SerperDevTool()

# Define Agents with enhanced capabilities (removed system_template parameter)
news_analyst = Agent(
    role="Senior News Analyst",
    goal="Extract and summarize the most important facts from news articles with precision and clarity",
    backstory="""You are a veteran news analyst with 20 years of experience at major publications. 
    You excel at quickly identifying the key facts in any article and presenting them clearly. 
    You focus on concrete information: who, what, when, where, why, and how. 
    You never use placeholder text and always analyze the actual content provided.
    
    When analyzing articles:
    1. Always work with the actual article content provided
    2. Extract specific facts, not generalizations
    3. Include names, numbers, dates, and locations when available
    4. Never use placeholder or template text
    5. Be concise but informative""",
    llm=llm,
    verbose=True,
    memory=False,
    allow_delegation=False,
    max_iter=1,
    max_rpm=8
)

cred_checker = Agent(
    role="Media Credibility Expert",
    goal="Evaluate the reliability and trustworthiness of news articles based on journalistic standards",
    backstory="""You are an expert in media literacy and journalism ethics with a background in fact-checking. 
    You evaluate articles based on source quality, attribution, evidence, and journalistic standards. 
    You provide balanced assessments backed by specific observations from the text.
    
    When assessing articles:
    1. Look for specific source attribution and quotes
    2. Check if claims are supported by evidence
    3. Evaluate the balance and fairness of reporting
    4. Consider the publication and author credibility
    5. Always base your assessment on the actual article content""",
    llm=llm,
    verbose=True,
    memory=False,
    tools=[search_tool],
    allow_delegation=False,
    max_iter=1,
    max_rpm=8
)

source_hunter = Agent(
    role="Fact Verification Specialist",
    goal="Verify key claims in articles using reliable sources and evidence",
    backstory="""You are a meticulous fact-checker who specializes in verifying claims using multiple sources. 
    You have extensive experience in investigative research and cross-referencing information. 
    You are skilled at using search tools to find corroborating or conflicting evidence.
    
    When verifying claims:
    1. Extract specific, verifiable claims from the article
    2. Use search tools to find supporting or conflicting evidence
    3. Prefer official sources and reputable publications
    4. Clearly state your verification findings
    5. Be thorough but concise in your explanations""",
    llm=llm,
    tools=[search_tool],
    verbose=True,
    memory=False,
    allow_delegation=False,
    max_iter=3,  # Allow 3 iterations for thorough search
    max_rpm=8
)

bias_detector = Agent(
    role="Media Bias Analyst",
    goal="Identify and analyze potential bias in news reporting through language and framing analysis",
    backstory="""You are a linguistics expert specializing in media analysis and bias detection. 
    You have studied how language choices, framing, and narrative structure can influence perception. 
    You provide objective analysis of potential bias while recognizing that all reporting has some perspective.
    
    When analyzing for bias:
    1. Examine specific word choices and their connotations
    2. Look for loaded language or emotional appeals
    3. Check if all relevant perspectives are included
    4. Identify what might be missing from the story
    5. Provide specific examples from the text""",
    llm=llm,
    verbose=True,
    memory=False,
    allow_delegation=False,
    max_iter=1,
    max_rpm=8
)

# Enhanced tasks with better context handling and examples
analysis_task = Task(
    description="""Analyze the provided article content and create 5 key bullet points summarizing the main facts.
    
    Article content: {article}
    
    Instructions:
    1. Read the entire article carefully
    2. Identify the 5 most important facts or developments
    3. Write each fact as a clear, concise bullet point
    4. Focus on concrete information (numbers, names, places, events)
    5. Each bullet should be 1-2 sentences maximum
    
    Format your response as:
    ‚Ä¢ [First key fact]
    ‚Ä¢ [Second key fact]
    ‚Ä¢ [Third key fact]
    ‚Ä¢ [Fourth key fact]
    ‚Ä¢ [Fifth key fact]
    
    Example output:
    ‚Ä¢ Canadian tourism to US border states has dropped by 50% since spring according to Kingdom Trails Association
    ‚Ä¢ Jay Peak Resort reports cancellations started after Trump's "51st state" comments about Canada
    ‚Ä¢ Vermont businesses are offering "at-par" pricing to attract Canadian customers back
    ‚Ä¢ Maine Governor installed bilingual "Bienvenue Canadiens" signs along interstates
    ‚Ä¢ 60% of Jay Peak's summer traffic typically comes from Canadian visitors""",
    expected_output="5 clear bullet points summarizing the article's main facts",
    agent=news_analyst
)

credibility_task = Task(
    description="""Assess the credibility of the article provided below.
    
    Article content: {article}
    
    Instructions:
    1. Evaluate the sources mentioned (are they named and reputable?)
    2. Check for specific data, quotes, and attribution
    3. Look for balanced perspectives or potential bias
    4. Assess factual accuracy based on your knowledge
    5. Rate credibility from 0-100 (0=not credible, 100=highly credible)
    
    Provide:
    - Credibility Score: [0-100]
    - Explanation: [2-3 sentences explaining your rating]""",
    expected_output="Credibility score (0-100) with 2-3 sentence explanation",
    agent=cred_checker
)

source_validation_task = Task(
    description="""Identify and verify the top 3 most important factual claims in the article.
    
    Article content: {article}
    
    Instructions:
    1. Identify the 3 most significant factual claims (statistics, quotes, events)
    2. Use web search to verify each claim if possible
    3. Mark each as: Verified (found supporting evidence), Unverified (no evidence found), or Disputed (conflicting information)
    4. Briefly explain your verification for each
    
    Format:
    1. [Claim]: [Status] - [Brief explanation]
    2. [Claim]: [Status] - [Brief explanation]  
    3. [Claim]: [Status] - [Brief explanation]""",
    expected_output="List of 3 claims with verification status and explanations",
    agent=source_hunter
)

bias_detection_task = Task(
    description="""Analyze the article for potential bias in language, framing, or perspective.
    
    Article content: {article}
    
    Instructions:
    1. Examine word choices and tone (emotional vs neutral language)
    2. Check if multiple viewpoints are presented fairly
    3. Look for missing context or one-sided reporting
    4. Identify any loaded language or persuasive techniques
    5. Rate bias from 0-100 (0=completely neutral, 100=extremely biased)
    
    Provide:
    - Bias Score: [0-100]
    - Examples: [If score > 20, provide 2 specific examples of biased language or framing]
    - If neutral (score ‚â§ 20), state "Article appears largely neutral in tone and presentation" """,
    expected_output="Bias score (0-100) with examples if biased",
    agent=bias_detector
)

def extract_article_text(html_content: str) -> str:
    """Extract article text from HTML content"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
            script.decompose()
        
        # Try to find main content area
        content_selectors = [
            'article',
            '[role="main"]',
            '.article-content',
            '.story-content',
            '.post-content',
            '.entry-content',
            '.content',
            'main'
        ]
        
        content_text = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    content_text += element.get_text()
                break
        
        # Fallback to body if no content area found
        if not content_text:
            body = soup.find('body')
            if body:
                content_text = body.get_text()
        
        # Clean up text
        lines = (line.strip() for line in content_text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        content_text = ' '.join(chunk for chunk in chunks if chunk)
        
        return content_text
    
    except Exception as e:
        print(f"Error extracting text: {e}")
        return ""

def preprocess_article(content: str) -> str:
    """Clean and preprocess article content"""
    if not content or len(content.strip()) < 50:
        raise ValueError("Article content is too short or empty")
    
    # Remove excessive whitespace
    content = re.sub(r'\s+', ' ', content)
    
    # Remove common website elements
    content = re.sub(r'(subscribe|newsletter|advertisement|cookie policy)', '', content, flags=re.IGNORECASE)
    
    # Basic cleaning
    content = content.strip()
    
    return content

def fetch_url(url: str) -> str:
    """Fetch content from URL"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        return resp.text
    except requests.exceptions.HTTPError as e:
        if e.response and e.response.status_code == 403:
            print("‚ùå Access forbidden. The website blocks automated access.")
            print("üí° Try copying and pasting the article text instead.")
        else:
            print(f"‚ùå HTTP error: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Failed to fetch URL: {e}")
        return None

def get_multiline_input():
    """Get multi-line input from user"""
    print("\nüìù Paste your article below.")
    print("When done, press Enter twice (two blank lines):\n")
    
    lines = []
    consecutive_empty = 0
    
    while True:
        try:
            line = input()
            
            if line == "":
                consecutive_empty += 1
                if consecutive_empty >= 2:
                    # Remove trailing empty lines
                    while lines and lines[-1] == "":
                        lines.pop()
                    break
                lines.append(line)
            else:
                consecutive_empty = 0
                lines.append(line)
                
        except EOFError:
            break
        except KeyboardInterrupt:
            print("\n\nInput cancelled.")
            return ""
    
    return "\n".join(lines)

def smart_truncate(content: str, max_chars: int = 6000) -> str:
    """Intelligently truncate content"""
    if len(content) <= max_chars:
        return content
    
    # Remove extra whitespace
    content = ' '.join(content.split())
    
    if len(content) <= max_chars:
        return content
    
    # Try to find a good break point
    truncated = content[:max_chars]
    
    # Look for sentence endings
    for delimiter in ['. ', '.\n', '! ', '? ', '\n\n']:
        last_pos = truncated.rfind(delimiter)
        if last_pos > max_chars * 0.8:  # Within 80% of limit
            return truncated[:last_pos + len(delimiter)].strip()
    
    # Fallback: break at word boundary
    last_space = truncated.rfind(' ')
    if last_space > max_chars * 0.8:
        return truncated[:last_space].strip()
    
    return truncated.strip()

def format_analysis_output(crew_result) -> dict:
    """Format the analysis output for better readability"""
    try:
        # Extract individual task results
        tasks_output = crew_result.tasks_output if hasattr(crew_result, 'tasks_output') else []
        
        formatted_result = {
            'key_points': '',
            'credibility': '',
            'fact_verification': '',
            'bias_analysis': ''
        }
        
        # Map each task output to the correct section
        task_names = ['analysis_task', 'credibility_task', 'source_validation_task', 'bias_detection_task']
        result_keys = ['key_points', 'credibility', 'fact_verification', 'bias_analysis']
        
        for i, task_output in enumerate(tasks_output):
            if i < len(result_keys):
                formatted_result[result_keys[i]] = str(task_output.raw)
        
        return formatted_result
    except:
        # Fallback to parsing the string output
        result_str = str(crew_result)
        return {
            'key_points': result_str,
            'credibility': '',
            'fact_verification': '',
            'bias_analysis': ''
        }

def print_colored_text(text: str, color: str = 'white'):
    """Print colored text to terminal"""
    colors = {
        'red': '\033[91m',
        'green': '\033[92m',
        'yellow': '\033[93m',
        'blue': '\033[94m',
        'purple': '\033[95m',
        'cyan': '\033[96m',
        'white': '\033[97m',
        'bold': '\033[1m',
        'end': '\033[0m'
    }
    print(f"{colors.get(color, '')}{text}{colors['end']}")

def create_markdown_report(formatted_result: dict, content_length: int, processing_time: float, article_url: str = None) -> str:
    """Create a formatted markdown report"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    markdown_content = f"""# üì∞ CredAI News Analysis Report

**Generated:** {timestamp}  
**Processing Time:** {processing_time//60:.0f}m {processing_time%60:.0f}s  
**Content Analyzed:** {content_length:,} characters  
"""
    
    if article_url:
        markdown_content += f"**Source URL:** {article_url}\n"
    
    markdown_content += "\n---\n\n"
    
    # Key Points Section
    if formatted_result['key_points']:
        markdown_content += "## üìå Key Points Summary\n\n"
        markdown_content += formatted_result['key_points'] + "\n\n"
    
    # Credibility Assessment
    if formatted_result['credibility']:
        markdown_content += "## üîç Credibility Assessment\n\n"
        markdown_content += formatted_result['credibility'] + "\n\n"
    
    # Fact Verification
    if formatted_result['fact_verification']:
        markdown_content += "## ‚úÖ Fact Verification\n\n"
        markdown_content += formatted_result['fact_verification'] + "\n\n"
    
    # Bias Analysis
    if formatted_result['bias_analysis']:
        markdown_content += "## ‚öñÔ∏è Bias Analysis\n\n"
        markdown_content += formatted_result['bias_analysis'] + "\n\n"
    
    markdown_content += "---\n\n*Report generated by CredAI News Analysis System*"
    
    return markdown_content

def main():
    # Check API keys
    if not os.getenv("GOOGLE_API_KEY"):
        print_colored_text("‚ùå ERROR: GOOGLE_API_KEY not found!", 'red')
        print_colored_text("\nPlease create a .env file with:", 'yellow')
        print_colored_text("GOOGLE_API_KEY=your_api_key_here", 'cyan')
        sys.exit(1)
    
    # Serper is optional
    if not os.getenv("SERPER_API_KEY"):
        print_colored_text("‚ö†Ô∏è  Warning: SERPER_API_KEY not found", 'yellow')
        print_colored_text("Web search features will be limited\n", 'yellow')
    
    # Create reports directory
    reports_dir = "reports"
    os.makedirs(reports_dir, exist_ok=True)
    
    # Enhanced header
    print_colored_text("\n" + "="*70, 'cyan')
    print_colored_text("ü§ñ CredAI News Analysis System v2.0", 'bold')
    print_colored_text("="*70, 'cyan')
    print_colored_text("‚úÖ Using Gemini 1.5 Flash with automatic rate limiting", 'green')
    print_colored_text("‚è±Ô∏è  Processing takes 3-5 minutes to respect API limits", 'blue')
    print_colored_text("üìÅ Reports saved in markdown format\n", 'purple')
    
    # Get input with better prompts
    print_colored_text("üìù Input Options:", 'bold')
    print_colored_text("  1Ô∏è‚É£  Paste article text", 'white')
    print_colored_text("  2Ô∏è‚É£  Enter article URL", 'white')
    
    choice = input("\nüîΩ Choose option (1 or 2): ").strip()
    article_url = None
    
    if choice == "1":
        content = get_multiline_input()
        if not content.strip():
            print_colored_text("\n‚ùå No content provided!", 'red')
            sys.exit(1)
    elif choice == "2":
        url = input("\nüåê Enter article URL: ").strip()
        article_url = url
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        print_colored_text("üîÑ Fetching content...", 'blue')
        html_content = fetch_url(url)
        if not html_content:
            print_colored_text("\nüí° Tip: Copy the article text from your browser and use option 1", 'yellow')
            sys.exit(1)
        
        # Extract text from HTML
        content = extract_article_text(html_content)
        if len(content) < 100:
            print_colored_text("‚ùå Could not extract article text from webpage", 'red')
            print_colored_text("üí° Try copying and pasting the article text instead", 'yellow')
            sys.exit(1)
    else:
        print_colored_text("‚ùå Invalid choice.", 'red')
        sys.exit(1)
    
    # Content statistics with better formatting
    original_length = len(content)
    print_colored_text(f"\nüìä Content Analysis:", 'bold')
    print_colored_text(f"   üìÑ Original length: {original_length:,} characters", 'white')
    
    # Smart truncation for API limits
    MAX_CHARS = 6000  # Conservative limit
    if original_length > MAX_CHARS:
        content = smart_truncate(content, MAX_CHARS)
        print_colored_text(f"   ‚úÇÔ∏è  Truncated to: {len(content):,} characters", 'yellow')
        print_colored_text("   üí° For full analysis of long articles:", 'cyan')
        print_colored_text("      - Break into sections", 'cyan')
        print_colored_text("      - Upgrade to paid API plan", 'cyan')
    
    # Verify content quality
    if len(content) < 100:
        print_colored_text("\n‚ùå Article too short for meaningful analysis", 'red')
        sys.exit(1)
    
    # Preprocess the article
    try:
        processed_content = preprocess_article(content)
    except ValueError as e:
        print_colored_text(f"\n‚ùå {e}", 'red')
        sys.exit(1)
    
    # Create crew with enhanced configuration
    crew = Crew(
        agents=[news_analyst, cred_checker, source_hunter, bias_detector],
        tasks=[analysis_task, credibility_task, source_validation_task, bias_detection_task],
        process=Process.sequential,
        verbose=False,  # Reduce verbose output for cleaner display
        memory=False,
        max_rpm=5,
        planning=False
    )
    
    try:
        print_colored_text("\n" + "="*70, 'cyan')
        print_colored_text("üîÑ Starting Analysis Pipeline", 'bold')
        print_colored_text("="*70, 'cyan')
        
        # Progress indicators
        steps = [
            "üìä Analyzing key facts",
            "üîç Assessing credibility", 
            "‚úÖ Verifying claims",
            "‚öñÔ∏è  Detecting bias"
        ]
        
        for step in steps:
            print_colored_text(f"‚è≥ {step}...", 'blue')
        
        print_colored_text("\n‚è±Ô∏è  Estimated time: 3-5 minutes", 'yellow')
        print()
        
        start_time = time.time()
        
        # Run analysis with processed content
        result = crew.kickoff(inputs={"article": processed_content})
        
        elapsed = time.time() - start_time
        
        # Format results
        formatted_result = format_analysis_output(result)
        
        # Display results with enhanced formatting
        print_colored_text("\n" + "="*70, 'green')
        print_colored_text("‚úÖ ANALYSIS COMPLETE", 'bold')
        print_colored_text("="*70, 'green')
        print_colored_text(f"‚è±Ô∏è  Processing time: {elapsed//60:.0f}m {elapsed%60:.0f}s", 'cyan')
        print()
        
        # Display each section with colors
        sections = [
            ("üìå KEY POINTS SUMMARY", 'key_points', 'blue'),
            ("üîç CREDIBILITY ASSESSMENT", 'credibility', 'green'), 
            ("‚úÖ FACT VERIFICATION", 'fact_verification', 'yellow'),
            ("‚öñÔ∏è BIAS ANALYSIS", 'bias_analysis', 'purple')
        ]
        
        for title, key, color in sections:
            if formatted_result[key]:
                print_colored_text(f"\n{title}", color)
                print_colored_text("-" * 50, color)
                print(formatted_result[key])
        
        print_colored_text("\n" + "="*70, 'green')
        
        # Save report as markdown
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"news_analysis_{timestamp}.md"
        filepath = os.path.join(reports_dir, filename)
        
        markdown_content = create_markdown_report(
            formatted_result, 
            len(content), 
            elapsed,
            article_url
        )
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(markdown_content)
        
        print_colored_text(f"üíæ Report saved: {filepath}", 'green')
        print_colored_text("‚ú® Analysis completed successfully!", 'bold')
        
    except KeyboardInterrupt:
        print_colored_text("\n\n‚ö†Ô∏è  Analysis interrupted by user", 'yellow')
        sys.exit(0)
    except Exception as e:
        error_msg = str(e)
        print_colored_text(f"\n‚ùå Error: {error_msg}", 'red')
        
        if "429" in error_msg or "rate limit" in error_msg.lower():
            print_colored_text("\nüö® RATE LIMIT ERROR DETECTED", 'red')
            print_colored_text("\nImmediate solutions:", 'yellow')
            print_colored_text("1. ‚è∞ Wait 60 seconds and try again", 'white')
            print_colored_text("2. üìÑ Use a shorter article", 'white')
            print_colored_text("3. üïí Try again in 1-2 minutes", 'white')
            print_colored_text("\nLong-term solutions:", 'cyan')
            print_colored_text("4. üí≥ Upgrade to Google AI Studio paid plan", 'white')
            print_colored_text("5. üîë Use a different API key", 'white')
            print_colored_text("6. üåê Try during off-peak hours", 'white')
        elif "API key" in error_msg:
            print_colored_text("\nüîë API KEY ERROR", 'red')
            print_colored_text("1. Verify your key at: https://aistudio.google.com/apikey", 'cyan')
            print_colored_text("2. Make sure it's a Google AI Studio key (not Cloud)", 'white')
            print_colored_text("3. Check if the key has expired", 'white')
        else:
            print_colored_text("\nüí° Troubleshooting tips:", 'yellow')
            print_colored_text("1. Check your internet connection", 'white')
            print_colored_text("2. Verify API keys in .env file", 'white')
            print_colored_text("3. Try with a shorter article", 'white')
            print_colored_text("4. Check API status at Google AI Studio", 'white')
        
        if "--debug" in sys.argv:
            print_colored_text("\nüìù Debug trace:", 'cyan')
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()